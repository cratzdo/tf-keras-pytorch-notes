{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_academic_dataset_business.json\n",
      "yelp_academic_dataset_review.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output(['ls', './data/']).decode('utf8'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text\n0  My wife took me here on my birthday for breakf...\n1  I have no idea why some people give bad review...\n2  love the gyro plate. Rice is so good and I als...\n3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n4  General Manager Scott Petello is a good egg!!!..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "review_file = 'data/yelp_academic_dataset_review.json'\n",
    "biz_file = 'data/yelp_academic_dataset_business.json'\n",
    "\n",
    "with open(review_file) as f:\n",
    "    js = []\n",
    "    for i in range(1000):\n",
    "        js.append(json.loads(f.readline()))\n",
    "\n",
    "review_df = pd.DataFrame(js)\n",
    "\n",
    "# take a look at the first five reviews\n",
    "review_df[['text']][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9925 65263 111809\n"
     ]
    }
   ],
   "source": [
    "#bag-of-words\n",
    "bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b') # unigram\n",
    "bigram_converter = CountVectorizer(ngram_range=(2, 2),\n",
    "                                   token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "trigram_converter = CountVectorizer(ngram_range=(3, 3),\n",
    "                                    token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "bow_converter.fit(review_df['text'])\n",
    "words = bow_converter.get_feature_names()\n",
    "\n",
    "bigram_converter.fit(review_df['text'])\n",
    "bigrams = bigram_converter.get_feature_names()\n",
    "\n",
    "trigram_converter.fit(review_df['text'])\n",
    "trigrams = trigram_converter.get_feature_names()\n",
    "\n",
    "print(len(words), len(bigrams), len(trigrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '00', '000', '00pm', '02']\n",
      "================================================================================\n",
      "['zucchini the', 'zucchini was', 'zuch and', 'zupas officially', 'zuzus room']\n",
      "================================================================================\n",
      "['zucchini the zucchini', 'zucchini was really', 'zuch and asparagus', 'zupas officially opens', 'zuzus room service']\n"
     ]
    }
   ],
   "source": [
    "print(words[:5])\n",
    "print('='*80)\n",
    "print(bigrams[-5:])\n",
    "print('='*80)\n",
    "print(trigrams[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " unique unigrams=9925\n",
      " unique bigrams=65263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111809"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\" unique unigrams={}\\n unique bigrams={}\".\n",
    "      format(len(set(words)), len(set(bigrams))))\n",
    "\n",
    "len(set(trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Use of Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stemmer.stem('zeroes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['General', 'PROPN', 'NNP']\n",
      "['Manager', 'PROPN', 'NNP']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['Petello', 'PROPN', 'NNP']\n",
      "['is', 'VERB', 'VBZ']\n",
      "['a', 'DET', 'DT']\n",
      "['good', 'ADJ', 'JJ']\n",
      "['egg', 'NOUN', 'NN']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['Not', 'ADV', 'RB']\n",
      "['to', 'PART', 'TO']\n",
      "['go', 'VERB', 'VB']\n",
      "['into', 'ADP', 'IN']\n",
      "['detail', 'NOUN', 'NN']\n",
      "[',', 'PUNCT', ',']\n",
      "['but', 'CCONJ', 'CC']\n",
      "['let', 'VERB', 'VB']\n",
      "['me', 'PRON', 'PRP']\n",
      "['assure', 'VERB', 'VB']\n",
      "['you', 'PRON', 'PRP']\n",
      "['if', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['have', 'VERB', 'VBP']\n",
      "['any', 'DET', 'DT']\n",
      "['issues', 'NOUN', 'NNS']\n",
      "['(', 'PUNCT', '-LRB-']\n",
      "['albeit', 'ADP', 'IN']\n",
      "['rare', 'ADJ', 'JJ']\n",
      "[')', 'PUNCT', '-RRB-']\n",
      "['speak', 'VERB', 'VBP']\n",
      "['with', 'ADP', 'IN']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['treat', 'VERB', 'VB']\n",
      "['the', 'DET', 'DT']\n",
      "['guy', 'NOUN', 'NN']\n",
      "['with', 'ADP', 'IN']\n",
      "['some', 'DET', 'DT']\n",
      "['respect', 'NOUN', 'NN']\n",
      "['as', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['state', 'VERB', 'VBP']\n",
      "['your', 'ADJ', 'PRP$']\n",
      "['case', 'NOUN', 'NN']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['I', 'PRON', 'PRP']\n",
      "[\"'d\", 'VERB', 'MD']\n",
      "['be', 'VERB', 'VB']\n",
      "['surprised', 'ADJ', 'JJ']\n",
      "['if', 'ADP', 'IN']\n",
      "['you', 'PRON', 'PRP']\n",
      "['do', 'VERB', 'VBP']\n",
      "[\"n't\", 'ADV', 'RB']\n",
      "['walk', 'VERB', 'VB']\n",
      "['out', 'ADV', 'RB']\n",
      "['totally', 'ADV', 'RB']\n",
      "['satisfied', 'ADJ', 'JJ']\n",
      "['as', 'ADP', 'IN']\n",
      "['I', 'PRON', 'PRP']\n",
      "['just', 'ADV', 'RB']\n",
      "['did', 'VERB', 'VBD']\n",
      "['.', 'PUNCT', '.']\n",
      "['Like', 'INTJ', 'UH']\n",
      "['I', 'PRON', 'PRP']\n",
      "['always', 'ADV', 'RB']\n",
      "['say', 'VERB', 'VBP']\n",
      "['.....', 'PUNCT', 'NFP']\n",
      "['\"', 'PUNCT', '``']\n",
      "['Mistakes', 'NOUN', 'NNS']\n",
      "['are', 'VERB', 'VBP']\n",
      "['inevitable', 'ADJ', 'JJ']\n",
      "[',', 'PUNCT', ',']\n",
      "['it', 'PRON', 'PRP']\n",
      "[\"'s\", 'VERB', 'VBZ']\n",
      "['how', 'ADV', 'WRB']\n",
      "['we', 'PRON', 'PRP']\n",
      "['recover', 'VERB', 'VBP']\n",
      "['from', 'ADP', 'IN']\n",
      "['them', 'PRON', 'PRP']\n",
      "['that', 'ADJ', 'WDT']\n",
      "['is', 'VERB', 'VBZ']\n",
      "['important', 'ADJ', 'JJ']\n",
      "['\"', 'PUNCT', \"''\"]\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['\\n\\n', 'SPACE', '_SP']\n",
      "['Thanks', 'NOUN', 'NNS']\n",
      "['to', 'ADP', 'IN']\n",
      "['Scott', 'PROPN', 'NNP']\n",
      "['and', 'CCONJ', 'CC']\n",
      "['his', 'ADJ', 'PRP$']\n",
      "['awesome', 'ADJ', 'JJ']\n",
      "['staff', 'NOUN', 'NN']\n",
      "['.', 'PUNCT', '.']\n",
      "['You', 'PRON', 'PRP']\n",
      "[\"'ve\", 'VERB', 'VB']\n",
      "['got', 'VERB', 'VBN']\n",
      "['a', 'DET', 'DT']\n",
      "['customer', 'NOUN', 'NN']\n",
      "['for', 'ADP', 'IN']\n",
      "['life', 'NOUN', 'NN']\n",
      "['!', 'PUNCT', '.']\n",
      "['!', 'PUNCT', '.']\n",
      "['..........', 'PUNCT', 'NFP']\n",
      "[':', 'PUNCT', ':']\n",
      "['^', 'PUNCT', 'NFP']\n",
      "[')', 'PUNCT', '-RRB-']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# use the first 10 reviews to exploration\n",
    "review_df2 = review_df[:10]\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc_df = review_df2['text'].apply(nlp)\n",
    "\n",
    "for doc in doc_df[4]:\n",
    "    print([doc.text, doc.pos_, doc.tag_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[General Manager Scott Petello, a good egg, detail, me, you, you, any issues, Scott, the guy, some respect, you, your case, I, you, I, I, Mistakes, it, we, them, Thanks, Scott, his awesome staff, You, a customer, life]\n"
     ]
    }
   ],
   "source": [
    "# noun chunking\n",
    "print([chunk for chunk in doc_df[4].noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Use `textblob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('General', 'NNP'),\n ('Manager', 'NNP'),\n ('Scott', 'NNP'),\n ('Petello', 'NNP'),\n ('is', 'VBZ'),\n ('a', 'DT'),\n ('good', 'JJ'),\n ('egg', 'NN'),\n ('Not', 'RB'),\n ('to', 'TO'),\n ('go', 'VB'),\n ('into', 'IN'),\n ('detail', 'NN'),\n ('but', 'CC'),\n ('let', 'VB'),\n ('me', 'PRP'),\n ('assure', 'VB'),\n ('you', 'PRP'),\n ('if', 'IN'),\n ('you', 'PRP'),\n ('have', 'VBP'),\n ('any', 'DT'),\n ('issues', 'NNS'),\n ('albeit', 'IN'),\n ('rare', 'NN'),\n ('speak', 'NN'),\n ('with', 'IN'),\n ('Scott', 'NNP'),\n ('and', 'CC'),\n ('treat', 'VB'),\n ('the', 'DT'),\n ('guy', 'NN'),\n ('with', 'IN'),\n ('some', 'DT'),\n ('respect', 'NN'),\n ('as', 'IN'),\n ('you', 'PRP'),\n ('state', 'NN'),\n ('your', 'PRP$'),\n ('case', 'NN'),\n ('and', 'CC'),\n ('I', 'PRP'),\n (\"'d\", 'MD'),\n ('be', 'VB'),\n ('surprised', 'VBN'),\n ('if', 'IN'),\n ('you', 'PRP'),\n ('do', 'VBP'),\n (\"n't\", 'RB'),\n ('walk', 'VB'),\n ('out', 'RP'),\n ('totally', 'RB'),\n ('satisfied', 'JJ'),\n ('as', 'IN'),\n ('I', 'PRP'),\n ('just', 'RB'),\n ('did', 'VBD'),\n ('Like', 'IN'),\n ('I', 'PRP'),\n ('always', 'RB'),\n ('say', 'VBP'),\n ('..', 'VBP'),\n ('Mistakes', 'NNS'),\n ('are', 'VBP'),\n ('inevitable', 'JJ'),\n ('it', 'PRP'),\n (\"'s\", 'VBZ'),\n ('how', 'WRB'),\n ('we', 'PRP'),\n ('recover', 'VBP'),\n ('from', 'IN'),\n ('them', 'PRP'),\n ('that', 'WDT'),\n ('is', 'VBZ'),\n ('important', 'JJ'),\n ('Thanks', 'NNS'),\n ('to', 'TO'),\n ('Scott', 'NNP'),\n ('and', 'CC'),\n ('his', 'PRP$'),\n ('awesome', 'JJ'),\n ('staff', 'NN'),\n ('You', 'PRP'),\n (\"'ve\", 'VBP'),\n ('got', 'VBN'),\n ('a', 'DT'),\n ('customer', 'NN'),\n ('for', 'IN'),\n ('life', 'NN'),\n ('^', 'NN')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob_df = review_df2['text'].apply(TextBlob)\n",
    "\n",
    "blob_df[4].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['general manager', 'scott petello', 'good egg', 'scott', \"n't walk\", '... ..', 'mistakes', 'thanks', 'scott', 'awesome staff', '... ... ...']\n"
     ]
    }
   ],
   "source": [
    "print([np for np in blob_df[4].noun_phrases])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Tf-Idf (term frequency-inverse documents frequency)\n",
    "\n",
    "- $bow(w, d)$ = #times words $w$ apprears in document d\n",
    "- $tf-idf(w, d)$ = $bow(w, d)*N$/(# documents in which word $w$ appears)\n",
    "\n",
    "> $N$ is the total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "with open(biz_file) as f:\n",
    "    biz_df = pd.DataFrame([json.loads(x) for x in f.readlines()])\n",
    "\n",
    "with open(review_file) as f:\n",
    "    review_df = pd.DataFrame([json.loads(x) for x in f.readlines()])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "two_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] or 'Restaurants' in x['categories'], axis=1)]\n",
    "\n",
    "twobiz_reviews = two_biz.merge(review_df, on='business_id', how='inner')\n",
    "\n",
    "# create a target column \n",
    "twobiz_reviews['target'] = twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          attributes             business_id  \\\n0  {'Take-out': True, 'Wi-Fi': 'no', 'Good For': ...  mQfT3JYu18HN22DVylcE7A   \n1  {'Take-out': True, 'Wi-Fi': 'no', 'Good For': ...  mQfT3JYu18HN22DVylcE7A   \n2  {'Take-out': True, 'Wi-Fi': 'no', 'Good For': ...  mQfT3JYu18HN22DVylcE7A   \n3  {'Take-out': True, 'Wi-Fi': 'no', 'Good For': ...  mQfT3JYu18HN22DVylcE7A   \n4  {'Take-out': True, 'Wi-Fi': 'no', 'Good For': ...  mQfT3JYu18HN22DVylcE7A   \n\n                                          categories     city  \\\n0  [Bakeries, Food, Breakfast & Brunch, Sandwiche...  Phoenix   \n1  [Bakeries, Food, Breakfast & Brunch, Sandwiche...  Phoenix   \n2  [Bakeries, Food, Breakfast & Brunch, Sandwiche...  Phoenix   \n3  [Bakeries, Food, Breakfast & Brunch, Sandwiche...  Phoenix   \n4  [Bakeries, Food, Breakfast & Brunch, Sandwiche...  Phoenix   \n\n                                 full_address  \\\n0  3134 E Indian School Rd\\nPhoenix, AZ 85018   \n1  3134 E Indian School Rd\\nPhoenix, AZ 85018   \n2  3134 E Indian School Rd\\nPhoenix, AZ 85018   \n3  3134 E Indian School Rd\\nPhoenix, AZ 85018   \n4  3134 E Indian School Rd\\nPhoenix, AZ 85018   \n\n                                               hours   latitude   longitude  \\\n0  {'Monday': {'close': '15:30', 'open': '07:30'}...  33.495216 -112.014454   \n1  {'Monday': {'close': '15:30', 'open': '07:30'}...  33.495216 -112.014454   \n2  {'Monday': {'close': '15:30', 'open': '07:30'}...  33.495216 -112.014454   \n3  {'Monday': {'close': '15:30', 'open': '07:30'}...  33.495216 -112.014454   \n4  {'Monday': {'close': '15:30', 'open': '07:30'}...  33.495216 -112.014454   \n\n            name neighborhoods  ...    state    type_x        date  \\\n0  Bertha's Café            []  ...       AZ  business  2011-09-27   \n1  Bertha's Café            []  ...       AZ  business  2011-04-17   \n2  Bertha's Café            []  ...       AZ  business  2012-01-02   \n3  Bertha's Café            []  ...       AZ  business  2008-10-29   \n4  Bertha's Café            []  ...       AZ  business  2011-12-18   \n\n                review_id stars_y  \\\n0  9QECdUlKowg20lRDo8FwTQ       5   \n1  N7j2J3V278BSyYBMXuSqcw       5   \n2  hQXhKJcQ7-bwSOEPGix3Wg       5   \n3  NzqSfv9KdF-8N57xrn1Pmw       5   \n4  0z51aynkyDSQX-cPHhJxBQ       5   \n\n                                                text  type_y  \\\n0  Our administrative assistant at work is a big ...  review   \n1  I live no where near this place & have gone ou...  review   \n2  I had initially checked this place out a coupl...  review   \n3  If you're looking for a lunch salad/Sandwich, ...  review   \n4  Killer sandwiches and dill potato salad that s...  review   \n\n                  user_id                                 votes target  \n0  2LCrT_BOJvGV4xi_lmoRcA  {'funny': 0, 'useful': 1, 'cool': 0}  False  \n1  GSZebDbIf0h_2MFv8xBGrw  {'funny': 0, 'useful': 1, 'cool': 0}  False  \n2  twZoTvWG92ntMVfEvbCnIg  {'funny': 0, 'useful': 0, 'cool': 0}  False  \n3  6uNeagh9ljbwV8XRUn_cYA  {'funny': 1, 'useful': 2, 'cool': 1}  False  \n4  -jgGsPzZ4W2TyGh7KOwl9Q  {'funny': 0, 'useful': 0, 'cool': 0}  False  \n\n[5 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twobiz_reviews.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "nlp_yelp.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
